---
description: DSPy BootstrapFewShot Optimizer Flow
globs: 
alwaysApply: false
---
> You are an expert in DSPy 3.0. You understand how to use the `BootstrapFewShot` optimizer to automatically create effective few-shot demonstrations for any `dspy.Module`.

## `BootstrapFewShot` Optimizer Flow

This optimizer automates the creation of high-quality, task-specific few-shot examples (demonstrations). It simulates a "teacher" model to generate traces for a "student" model to learn from.

```
┌──────────────────┐    ┌──────────────────┐    ┌──────────────────┐
│ Student Program  │    │  Training Data   │    │  Evaluation     │
│  (No Examples)   │    │ (High-Quality)   │    │  Metric          │
└──────────────────┘    └──────────────────┘    └──────────────────┘
         │                       │                       │
         └───────────┬───────────┘                       │
                     ▼                                   │
┌────────────────────────────────────────────────────────▼──┐
│             BootstrapFewShot.compile(...)                 │
│  1. Teacher generates traces for each module on trainset. │
│  2. Finds best demonstrations for the student.            │
└───────────────────────────────────────────────────────────┘
                                  │
                                  ▼
┌───────────────────────────────────────────────────────────┐
│                    Optimized Program                      │
│ (The student program, now with effective few-shot         │
│         demonstrations compiled into its prompts)         │
└───────────────────────────────────────────────────────────┘
```

## Core Implementation Pattern

The most common use of `BootstrapFewShot` is to take a program with no hardcoded examples and compile it into a program that has effective few-shot examples for each predictor.

```python
import dspy
from dspy.teleprompt import BootstrapFewShot
from dspy.evaluate import answer_exact_match

# 1. Define your program (the "student")
class AnswerWithReason(dspy.Module):
    def __init__(self):
        super().__init__()
        # These modules have no few-shot examples yet
        self.generate_answer = dspy.ChainOfThought("question -> answer")

    def forward(self, question):
        return self.generate_answer(question=question)

# 2. Prepare your data and metric
trainset = [
    dspy.Example(question="What is the color of a ripe banana?", answer="Yellow").with_inputs("question"),
    dspy.Example(question="What is 9 * 8?", answer="72").with_inputs("question")
]
metric = answer_exact_match

# 3. Configure the optimizer
# - `metric`: The function to maximize.
# - `max_bootstrapped_demos`: The number of examples to generate for each module.
# - `teacher_settings`: Specifies the LM used to generate the "gold" traces.
config = dict(
    max_bootstrapped_demos=4, 
    teacher_settings=dict(lm=dspy.OpenAI(model='o3'))
)
optimizer = BootstrapFewShot(metric=metric, **config)

# 4. Compile the program
student_program = AnswerWithReason()
teacher_program = optimizer.compile(student_program, trainset=trainset)

# The 'teacher_program' is now a new instance of AnswerWithReason,
# but its `generate_answer` module contains 4 few-shot examples
# that were generated by the o3 teacher.
# When you call teacher_program, these examples will be included in the prompt.
```

## How `BootstrapFewShot` Works

1.  **Teacher Model**: It uses a powerful "teacher" model (like o3) to run your program on each example in your `trainset`. This generates high-quality "gold standard" traces, including reasoning steps for modules like `dspy.ChainOfThought`.
2.  **Demonstration Mining**: For each predictor in your program, the optimizer searches through the generated traces to find the most effective examples to use as demonstrations.
3.  **Compilation**: It creates a *new* instance of your program and injects the selected demonstrations into each predictor module. The `train` argument of `dspy.Predict` or `dspy.ChainOfThought` is now populated.

This compiled program is now a "few-shot" program, capable of producing much more accurate and well-formatted outputs.

## Advanced Usage

### Compiling Multi-Module Programs

`BootstrapFewShot` works seamlessly with complex programs that have multiple modules. It will generate and select demonstrations for *each* `dspy.Predict` or `dspy.ChainOfThought` instance independently.

```python
class MultiStepQA(dspy.Module):
    def __init__(self):
        super().__init__()
        self.get_topic = dspy.Predict("question -> topic")
        self.get_answer = dspy.ChainOfThought("topic -> answer")

    def forward(self, question):
        topic = self.get_topic(question=question).topic
        return self.get_answer(topic=topic)

# When you compile this program with BootstrapFewShot...
# optimizer.compile(MultiStepQA(), trainset=...)

# ...the resulting program will have:
# - Few-shot examples for `self.get_topic`
# - *Different* few-shot examples for `self.get_answer`
# Each set of examples is tailored to its specific module's signature.
```

### Freezing a Teacher

The "teacher" model can be expensive. If your training set is large, you can generate the "gold" traces once and save them, then run the optimization process multiple times with different parameters without re-running the teacher.

```python
# First, generate and save the teacher's traces
teacher = dspy.OpenAI(model='o3')
# This creates a list of (example, trace) pairs
traces = dspy.teleprompt.bootstrap_traces(trainset, teacher) 

# Now, create an optimizer that uses these pre-generated traces
# The `teacher_settings` are no longer needed.
optimizer = BootstrapFewShot(metric=metric, max_bootstrapped_demos=4)
compiled_program = optimizer.compile(student_program, trainset=traces)
```

## Best Practices Summary

- **High-Quality Teacher**: Use your best, most powerful model (e.g., o3, Claude 3 Opus) as the teacher. The quality of the demonstrations depends entirely on the teacher's outputs.
- **High-Quality `trainset`**: The training examples should be representative of your task and have accurate labels. Garbage in, garbage out.
- **`max_bootstrapped_demos`**: This parameter controls the number of examples in the final prompt. A good starting point is between 2 and 8. Too many can exceed context limits.
- **Separate Student and Teacher**: While not required, you can explicitly define a cheaper "student" LM for the final compiled program, which learns from the powerful "teacher" LM. This is a great cost-saving pattern.

## References
- [Rule: `04_Optimizers_Overview.mdc`](mdc:DSPy/04_Optimizers_Overview.mdc)
- [DSPy Documentation: BootstrapFewShot](mdc:link)
