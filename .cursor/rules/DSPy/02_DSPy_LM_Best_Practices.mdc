---
description: DSPy LM Best Practices Flow
globs: 
alwaysApply: false
---
> You are an expert in DSPy 3.0. You provide clear, actionable advice on the best practices for using Language Models (LMs) effectively and efficiently in DSPy programs.

## LM Best Practices Flow

```
┌──────────────────┐    ┌───────────────────┐    ┌───────────────────┐
│   Select Model   │    │  Control Behavior │    │ Manage API Usage  │
│ - Cost vs. Perf. │───▶│ - Temperature    │───▶│ - Rate Limits   │
│ - Task Alignment │    │ - Top-p, Top-k     │    │ - Caching       │
└──────────────────┘    └───────────────────┘    └───────────────────┘
         │                       │                       │
         ▼                       ▼                       ▼
┌──────────────────┐    ┌───────────────────┐    ┌───────────────────┐
│ Ensure Stability │    │  Handle Failures  │    │  Optimize Costs   │
│ - Reproducibility│───▶│ - Retries        │───▶│ - Model Tiering │
│ - Versioning     │    │ - Fallbacks      │    │ - Max Tokens    │
└──────────────────┘    └───────────────────┘    └───────────────────┘
```

## Core Best Practices

### Controlling LM Behavior

Parameters like `temperature` control the randomness and creativity of the LM's output.

```python
import dspy

# ✅ DO: Use low temperature for deterministic, factual tasks
factual_lm = dspy.OpenAI(
    model="gpt-4o",
    temperature=0.0,
    max_tokens=500
)

# ✅ DO: Use higher temperature for creative, brainstorming tasks
creative_lm = dspy.OpenAI(
    model="gpt-4o",
    temperature=0.7,
    top_p=0.9,
    max_tokens=2000
)

dspy.settings.configure(lm=factual_lm)

with dspy.context(lm=creative_lm):
    # This module will generate more diverse, creative outputs
    brainstormer = dspy.ChainOfThought("topic -> ideas")
    response = brainstormer(topic="New features for a note-taking app")

# ❌ DON'T: Use high temperature for tasks requiring precision (e.g., code generation, fact extraction).
# This can lead to hallucinations and incorrect results.
```

### Managing API Usage and Rate Limits

API calls can be costly and are often rate-limited. Proper management is crucial.

```python
import dspy
import time
from dspy.teleprompt import BootstrapFewShot

# ✅ DO: Implement exponential backoff for retries
# Many LM clients (like OpenAI's) handle this automatically.
# If not, you can add it to a custom LM wrapper.
class ResilientLM(dspy.LM):
    def __init__(self, base_lm):
        self.base_lm = base_lm

    def __call__(self, prompt, **kwargs):
        retries = 3
        delay = 1.0
        for i in range(retries):
            try:
                return self.base_lm(prompt, **kwargs)
            except SomeRateLimitError as e:
                if i == retries - 1:
                    raise e
                time.sleep(delay)
                delay *= 2.0

# ✅ DO: Use DSPy's built-in caching during development and optimization
# This saves time and money by avoiding repeated API calls for the same input.
dspy.settings.configure(
    lm=dspy.OpenAI(model="o3"),
    cache_turn_on=True,
    cache_dir="./.dspy_cache"
)

# ❌ DON'T: Ignore rate limits. This can lead to application failures.
# ❌ DON'T: Forget to turn off or manage caching in production if stale data is a concern.
```

## Advanced Best Practices

### Ensuring Reproducibility

For consistent outputs, especially in testing and production, it's vital to control the factors that lead to variability.

```python
# ✅ DO: Set a fixed temperature of 0.0 for maximum reproducibility.
reproducible_lm = dspy.OpenAI(model="gpt-4o", temperature=0.0)

# ✅ DO: Pin model versions.
# Models like "gpt-4-turbo" are aliases that get updated.
# For production, use a specific version like "gpt-4-turbo-2024-04-09".
prod_lm = dspy.OpenAI(model="gpt-4-turbo-2024-04-09", temperature=0.0)

# ✅ DO: Save and load optimized programs.
# Once a program is optimized, save its state. This freezes the prompts and demonstrations.
# See 07-Saving_and_Loading.mdc for more details.
# optimizer = BootstrapFewShot(metric=my_metric)
# optimized_program = optimizer.compile(my_program, trainset=trainset)
# optimized_program.save("my_optimized_program.json")
# loaded_program = MyProgram()
# loaded_program.load("my_optimized_program.json")


# ❌ DON'T: Rely on default model aliases in production environments where consistency is key.
```

### Cost Optimization

Strategically choosing models and parameters can significantly reduce costs.

```python
# ✅ DO: Use model tiering. Use cheaper models for simpler tasks.
fast_lm = dspy.OpenAI(model="o3") # For drafting, simple classification
powerful_lm = dspy.OpenAI(model="gpt-4o") # For complex reasoning, final checks

class TieredRAG(dspy.Module):
    def __init__(self):
        super().__init__()
        self.draft = dspy.Predict("question -> draft_answer", lm=fast_lm)
        self.refine = dspy.Predict("draft_answer -> refined_answer", lm=powerful_lm)

    def forward(self, question):
        draft = self.draft(question=question)
        refined = self.refine(draft_answer=draft.draft_answer)
        return refined

# ✅ DO: Set a reasonable `max_tokens` limit.
# This prevents unexpectedly long (and expensive) outputs.
# It also helps prevent the model from rambling.
limited_lm = dspy.OpenAI(model="o3", max_tokens=250)

# ❌ DON'T: Use the most powerful model for every single task. This is inefficient and expensive.
```

### Handling Different LM Providers

Different providers may have different optimal settings and kwargs.

```python
# ✅ DO: Abstract away provider-specific settings when possible.
def get_lm_for_provider(provider_name: str, **kwargs):
    if provider_name == "openai":
        return dspy.OpenAI(model="gpt-4o", **kwargs)
    elif provider_name == "anthropic":
        # Note: Anthropic uses 'max_tokens_to_sample'
        kwargs["max_tokens_to_sample"] = kwargs.pop("max_tokens", 1024)
        return dspy.Anthropic(model="claude-3-opus-20240229", **kwargs)
    # ... other providers

# ✅ DO: Be aware of different prompting strategies.
# Some models (like older versions of Claude) are very sensitive to prompt formatting.
# While DSPy abstracts much of this, complex custom prompts may need tuning.
# For example, Anthropic models work well with XML-style tags like <example>.
```

## Best Practices Summary

### Key Takeaways
- **Temperature**: `0.0` for facts, `0.7+` for creativity.
- **Reproducibility**: Pin model versions and set `temperature=0.0`.
- **Cost**: Use tiered models. Cheaper models for easy tasks, powerful models for hard tasks.
- **API Usage**: Use DSPy's built-in caching in development. Implement retry logic for production.
- **Parameters**: Set `max_tokens` to control output length and cost.
- **Provider Differences**: Be mindful of different kwargs and prompting needs between providers like OpenAI and Anthropic.

## References
- [DSPy Documentation: Language Models](mdc:link)
- [OpenAI API Reference: Chat](mdc:link)
- [Anthropic Documentation: Prompting](mdc:link)
