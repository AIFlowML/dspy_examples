---
description: DSPy Overview of Standard Metrics
globs: 
alwaysApply: false
---
> You are an expert in DSPy 3.0. You know how to select and apply the right built-in metrics to accurately measure program performance for a variety of common NLP tasks.

## Overview of Standard Metrics

DSPy provides several convenient, off-the-shelf metrics in the `dspy.evaluate.metrics` module. These are designed to cover the most common evaluation scenarios, like question answering and text generation.

The core signature for any metric is `metric(example: dspy.Example, prediction: dspy.Prediction, trace=None) -> bool | float`.

## Key Built-in Metrics

### `answer_exact_match`

This is the strictest string-based metric. It checks if the predicted answer is an exact string match to the gold answer.

- **Use Case**: Ideal for tasks with a single, unambiguous correct answer, like closed-book QA, fact-checking, or multiple-choice questions.
- **Returns**: `True` or `False`.

```python
import dspy
from dspy.evaluate.metrics import answer_exact_match

# Example where it works well
gold = dspy.Example(answer="42")
pred_correct = dspy.Prediction(answer="42")
print(answer_exact_match(gold, pred_correct)) # -> True

# Example where it is too strict
gold_flexible = dspy.Example(answer="The answer is forty-two.")
pred_flexible = dspy.Prediction(answer="forty-two")
print(answer_exact_match(gold_flexible, pred_flexible)) # -> False (fails due to extra words)
```

### `answer_f1`

This metric calculates the F1 score based on the overlap of words between the predicted and gold answers. It's more forgiving than exact match.

- **Use Case**: Excellent for open-ended QA or summarization where the exact wording can vary, but the key information should be present.
- **Returns**: A float between `0.0` and `1.0`.

```python
from dspy.evaluate.metrics import answer_f1

# ✅ DO: Use F1 for open-ended tasks
gold = dspy.Example(answer="The mitochondria is the powerhouse of the cell.")
pred = dspy.Prediction(answer="The cell's powerhouse is the mitochondria.")
print(answer_f1(gold, pred)) # -> 1.0 (perfect score despite different word order)

pred_partial = dspy.Prediction(answer="The mitochondria is a part of the cell.")
print(answer_f1(gold, pred_partial)) # -> ~0.6 (partial score)
```

### `answer_passage_match`

This metric checks if the gold answer appears as a substring within any of the passages in the retrieved context.

- **Use Case**: Specifically for open-domain QA where the answer is expected to be directly extracted from a retrieved document. It helps validate the retrieval step.
- **Returns**: `True` or `False`.

```python
from dspy.evaluate.metrics import answer_passage_match

gold = dspy.Example(answer="Marie Curie")
# The prediction must contain a `context` attribute with the retrieved passages.
pred_with_context = dspy.Prediction(
    context=["Maria Sklodowska, known as Marie Curie, was a Polish physicist...", "Other scientists..."]
)
print(answer_passage_match(gold, pred_with_context)) # -> True

pred_no_context = dspy.Prediction(context=["Albert Einstein was a physicist...", "..."])
print(answer_passage_match(gold, pred_no_context)) # -> False
```

## Using Metrics in an Evaluator

You can easily plug these metrics into the `dspy.evaluate.Evaluate` utility.

```python
import dspy
from dspy.evaluate import Evaluate
from dspy.evaluate.metrics import answer_exact_match, answer_f1

# ... (define your program and testset) ...

# ✅ DO: Pass the metric function directly to the evaluator
f1_evaluator = Evaluate(devset=testset, metric=answer_f1)
f1_score = f1_evaluator(my_program)
print(f"Average F1 Score: {f1_score}")

# ✅ DO: Evaluate on multiple standard metrics at once
multi_metric_evaluator = Evaluate(devset=testset, metric=[answer_exact_match, answer_f1])
scores = multi_metric_evaluator(my_program)
print(scores) # -> {'answer_exact_match': 0.8, 'answer_f1': 0.87}
```

## Best Practices Summary

- **Start with F1**: For most text-based tasks, `answer_f1` is a more robust and forgiving starting point than `answer_exact_match`.
- **Match Metric to Task**:
    - **Closed QA**: `answer_exact_match` is often sufficient.
    - **Open QA / Summarization**: `answer_f1` is a better choice.
    - **RAG Validation**: Use `answer_passage_match` to ensure your retriever is finding the correct documents.
- **Combine Metrics**: Use a list of metrics to get a more complete picture of your program's performance. For a RAG system, you might evaluate using `[answer_f1, answer_passage_match]` to check both the final answer quality and the retrieval quality.
- **Know Their Limits**: These standard metrics are based on string matching. They don't understand semantics. A predicted answer like "the creator of Python" will not match the gold answer "Guido van Rossum" even though they are semantically similar. For that, you need a custom, LM-based metric.

## References
- [Rule: `05_Evaluation_Workflow.mdc`](mdc:DSPy/05_Evaluation_Workflow.mdc)
- [Rule: `05_Custom_Metrics.mdc`](mdc:DSPy/05_Custom_Metrics.mdc)
- [DSPy Documentation: Metrics](mdc:link)
