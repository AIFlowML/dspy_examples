---
description: DSPy - Models usable in LiteLLM
globs: 
alwaysApply: false
---
> You are an expert in DSPy 3.0 and LiteLLM integration. You understand how to configure and use any of the 100+ LLM providers supported through LiteLLM's unified API in DSPy.

## DSPy with LiteLLM: Universal Model Access

DSPy leverages LiteLLM's extensive provider ecosystem, giving you access to 100+ different LLM providers through a single, consistent interface. This makes it incredibly easy to switch between providers, compare models, and build robust applications.

### Quick Start Pattern

```python
import dspy

# Universal pattern: dspy.LM('provider/model-name')
lm = dspy.LM('openai/gpt-4o-mini')
dspy.configure(lm=lm)
```

---

## 🔑 Authentication Methods

DSPy supports multiple authentication approaches:

1. **Environment Variables** (Recommended for security)
2. **Direct API Key Parameter** (For dynamic configuration)
3. **Configuration Files** (Provider-specific)

---

## 🚀 Major Providers & Models

### OpenAI
**Latest Models**: GPT-4o, GPT-4o-mini, GPT-4-turbo, o1-preview, o1-mini, GPT-3.5-turbo

| Model | Context Window | Max Output | Best For |
|-------|----------------|------------|----------|
| `gpt-4o` | 128K tokens | 16K tokens | Most capable, multimodal |
| `gpt-4o-mini` | 128K tokens | 16K tokens | Fast, cost-effective |
| `03` | 200K tokens | 100K tokens | Fast reasoning |
| `o3-mini` | 200K tokens | 100K tokens | Cost effective reasoning |

```python
import os
import dspy

# Method 1: Environment Variable (Recommended)
os.environ['OPENAI_API_KEY'] = 'your-api-key-here'

# Most capable model
lm_best = dspy.LM('openai/gpt-4o')

# Cost-effective option
lm_mini = dspy.LM('openai/gpt-4o-mini')

# Latest reasoning models
lm_reasoning = dspy.LM('openai/o1-preview')
lm_reasoning_fast = dspy.LM('openai/o1-mini')

# Legacy but cost-effective
lm_legacy = dspy.LM('openai/gpt-3.5-turbo')

# Method 2: Direct API Key
lm = dspy.LM('openai/gpt-4o-mini', api_key='your-api-key-here')

dspy.configure(lm=lm_mini)
```

**Environment Variables:**
- `OPENAI_API_KEY`

### Anthropic (Claude)
**Latest Models**: Claude-3.5-Sonnet, Claude-3.5-Haiku, Claude-3-Opus

| Model | Context Window | Max Output | Best For |
|-------|----------------|------------|----------|
| `claude-3-5-sonnet-20241022` | 200K tokens | 8K tokens | Most capable, coding |
| `claude-3-5-haiku-20241022` | 200K tokens | 8K tokens | Fast, cost-effective |
| `claude-3-opus-20240229` | 200K tokens | 4K tokens | Most intelligent |
| `claude-3-sonnet-20240229` | 200K tokens | 4K tokens | Balanced performance |
| `claude-3-haiku-20240307` | 200K tokens | 4K tokens | Fastest, cheapest |

```python
import dspy

os.environ['ANTHROPIC_API_KEY'] = 'your-api-key-here'

# Latest and most capable
lm_best = dspy.LM('anthropic/claude-3-5-sonnet-20241022')

# Fast and efficient
lm_fast = dspy.LM('anthropic/claude-3-5-haiku-20241022')

# Most intelligent (legacy)
lm_opus = dspy.LM('anthropic/claude-3-opus-20240229')

# Balanced options
lm_sonnet = dspy.LM('anthropic/claude-3-sonnet-20240229')
lm_haiku = dspy.LM('anthropic/claude-3-haiku-20240307')

dspy.configure(lm=lm_best)
```

**Environment Variables:**
- `ANTHROPIC_API_KEY`

### Google (Gemini/Vertex AI)
**Multimodal Models**: Gemini 2.5 Flash, Gemini 2.0 Flash, Gemini Pro

```python
import dspy

# Google AI Studio
os.environ['GEMINI_API_KEY'] = 'your-api-key-here'

# Latest multimodal with adaptive thinking
lm_latest = dspy.LM('gemini/gemini-2.5-flash-preview-05-20')

# Advanced reasoning
lm_pro = dspy.LM('gemini/gemini-2.5-pro-preview-06-05')

# Fast processing
lm_fast = dspy.LM('gemini/gemini-2.0-flash-exp')

# Text-to-speech capability
lm_tts = dspy.LM('gemini/gemini-2.5-flash-preview-tts')

# Vertex AI (Enterprise)
os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = 'path/to/service-account.json'
os.environ['VERTEXAI_PROJECT'] = 'your-project-id'
lm_vertex = dspy.LM('vertex_ai/gemini-2.0-flash')

dspy.configure(lm=lm_latest)
```

### Groq (High-Speed Inference)
**Production Models** with ultra-fast token generation:

| Model Name | Model ID | Context Window | Max Completion | Speed | Details |
|------------|----------|----------------|----------------|-------|---------|
| Gemma2 9B | `gemma2-9b-it` | 8,192 tokens | - | Fast | Google model |
| Llama Guard 4 | `meta-llama/llama-guard-4-12b` | 131,072 tokens | 128 tokens | Fast | Safety model |
| Llama 3.3 70B | `llama-3.3-70b-versatile` | 128K tokens | 32,768 tokens | ~2,100 tokens/s | Most capable |
| Llama 3.1 8B | `llama-3.1-8b-instant` | 128K tokens | 8,192 tokens | ~2,200 tokens/s | Fastest |
| Whisper Large v3 | `whisper-large-v3` | - | - | Fast | Audio transcription (25MB max) |
| Whisper Large v3 Turbo | `whisper-large-v3-turbo` | - | - | Faster | Fast audio transcription (25MB max) |
| Qwen QwQ 32B | `qwen-qwq-32b` | 128K tokens | - | Fast | Reasoning model |
| Qwen 3 32B | `qwen/qwen3-32b` | 128K tokens | 16,384 tokens | Fast | Latest Qwen model |

```python
import dspy

os.environ['GROQ_API_KEY'] = 'your-api-key-here'

# Fastest for simple tasks (~2,200 tokens/s)
lm_fastest = dspy.LM('groq/llama-3.1-8b-instant')

# Most capable (~2,100 tokens/s)
lm_capable = dspy.LM('groq/llama-3.3-70b-versatile')

# Reasoning models
lm_reasoning = dspy.LM('groq/qwen-qwq-32b')
lm_qwen = dspy.LM('groq/qwen/qwen3-32b')

# Google model
lm_gemma = dspy.LM('groq/gemma2-9b-it')

# Safety model
lm_guard = dspy.LM('groq/meta-llama/llama-guard-4-12b')

# Audio transcription
lm_whisper = dspy.LM('groq/whisper-large-v3-turbo')
lm_whisper_v3 = dspy.LM('groq/whisper-large-v3')

dspy.configure(lm=lm_fastest)
```

**Environment Variables:**
- `GROQ_API_KEY`

### Cerebras (Ultra-Fast Inference)
**Speed Champions** optimized for ultra-fast token generation:

| Model Name | Model ID | Parameters | Speed | Context Window | Notes |
|------------|----------|------------|-------|----------------|-------|
| Llama 4 Scout | `llama-4-scout-17b-16e-instruct` | 109B | ~2,600 tokens/s | - | Most advanced |
| Llama 3.1 8B | `llama3.1-8b` | 8B | ~2,200 tokens/s | - | Fastest |
| Llama 3.3 70B | `llama-3.3-70b` | 70B | ~2,100 tokens/s | - | Most capable |
| Qwen 3 32B* | `qwen-3-32b` | 32B | ~2,100 tokens/s | - | Hybrid reasoning |
| DeepSeek R1 Distill** | `deepseek-r1-distill-llama-70b` | 70B | ~1,700 tokens/s | - | Private preview |

*Qwen 3 is a hybrid reasoning model. Use `/no_think` in prompt to disable reasoning mode.  
**DeepSeek R1 Distill is in private preview - contact Cerebras for access.

```python
import dspy

os.environ['CEREBRAS_API_KEY'] = 'your-api-key-here'

# Latest Llama 4 Scout (~2,600 tokens/s)
lm_advanced = dspy.LM('cerebras/llama-4-scout-17b-16e-instruct')

# Fastest general model (~2,200 tokens/s)
lm_fastest = dspy.LM('cerebras/llama3.1-8b')

# Most capable (~2,100 tokens/s)
lm_capable = dspy.LM('cerebras/llama-3.3-70b')

# Hybrid reasoning (use /no_think to disable reasoning)
lm_reasoning = dspy.LM('cerebras/qwen-3-32b')

# Private preview model (requires access)
lm_distill = dspy.LM('cerebras/deepseek-r1-distill-llama-70b')

dspy.configure(lm=lm_fastest)
```

**Environment Variables:**
- `CEREBRAS_API_KEY`

### DeepSeek (Advanced Reasoning)
**Cost-Optimized Models** with dynamic pricing and time-based discounts:

| Model | Model ID | Context Length | Max Output | Features |
|-------|----------|----------------|------------|----------|
| DeepSeek Chat | `deepseek-chat` | 64K tokens | Default: 4K<br>Max: 8K | JSON Output, Function Calling, Chat Prefix |
| DeepSeek Reasoner | `deepseek-reasoner` | 64K tokens | Default: 32K<br>Max: 64K | Advanced reasoning, JSON Output, Function Calling |

**Pricing Features:**
- **Cache Hit Optimization**: Reduced costs for repeated content
- **Time-based Discounts**: 50-75% off during UTC 16:30-00:30
- **Standard Hours**: UTC 00:30-16:30 (full price)
- **Discount Hours**: UTC 16:30-00:30 (50-75% off)

```python
import dspy

os.environ['DEEPSEEK_API_KEY'] = 'your-api-key-here'

# General purpose (64K context, up to 8K output)
lm_chat = dspy.LM('deepseek/deepseek-chat', max_tokens=4000)

# Advanced reasoning (64K context, up to 64K output)
lm_reasoner = dspy.LM('deepseek/deepseek-reasoner', max_tokens=32000)

# With JSON output support
lm_json = dspy.LM('deepseek/deepseek-chat', 
                  max_tokens=4000,
                  response_format={"type": "json_object"})

dspy.configure(lm=lm_chat)
```

**Environment Variables:**
- `DEEPSEEK_API_KEY`

**Cost Optimization Tips:**
- Use during discount hours (UTC 16:30-00:30) for 50-75% savings
- Leverage cache hits for repeated queries
- Choose `deepseek-chat` for general tasks, `deepseek-reasoner` for complex reasoning

### Databricks
**Models**: Meta-Llama-3.1-70B-Instruct, DBRX-Instruct, etc.

```python
import dspy

# On Databricks platform (automatic authentication)
lm = dspy.LM('databricks/databricks-meta-llama-3-1-70b-instruct')

# External access
os.environ['DATABRICKS_API_KEY'] = 'your-api-key-here'
os.environ['DATABRICKS_API_BASE'] = 'your-workspace-url'
lm = dspy.LM('databricks/databricks-meta-llama-3-1-70b-instruct')

dspy.configure(lm=lm)
```

**Environment Variables:**
- `DATABRICKS_API_KEY`
- `DATABRICKS_API_BASE`

### Azure OpenAI
**Enterprise Models**: GPT-4o, GPT-4-turbo, GPT-3.5-turbo (deployed models)

```python
import dspy

os.environ['AZURE_API_KEY'] = 'your-api-key-here'
os.environ['AZURE_API_BASE'] = 'https://your-resource.openai.azure.com'
os.environ['AZURE_API_VERSION'] = '2024-02-15-preview'

lm = dspy.LM('azure/your-deployment-name')

# Or pass directly
lm = dspy.LM('azure/your-deployment-name', 
             api_key='your-api-key-here',
             api_base='https://your-resource.openai.azure.com',
             api_version='2024-02-15-preview')

dspy.configure(lm=lm)
```

**Environment Variables:**
- `AZURE_API_KEY`
- `AZURE_API_BASE`
- `AZURE_API_VERSION`
- `AZURE_AD_TOKEN` (optional)
- `AZURE_API_TYPE` (optional)

### Together AI
**Models**: Llama-2-70B-Chat, Mixtral-8x7B, etc.

```python
import dspy

os.environ['TOGETHERAI_API_KEY'] = 'your-api-key-here'
lm = dspy.LM('together_ai/togethercomputer/llama-2-70b-chat')

dspy.configure(lm=lm)
```

**Environment Variables:**
- `TOGETHERAI_API_KEY`

### Anyscale
**Models**: Mistral-7B-Instruct, Llama-2-70B-Chat, etc.

```python
import dspy

os.environ['ANYSCALE_API_KEY'] = 'your-api-key-here'
lm = dspy.LM('anyscale/mistralai/Mistral-7B-Instruct-v0.1')

dspy.configure(lm=lm)
```

**Environment Variables:**
- `ANYSCALE_API_KEY`

---

## 🛠️ Advanced Configuration

### Multi-Model Setup
Use different models for different tasks:

```python
import dspy

# Configure specialized models
models = {
    'fast': dspy.LM('groq/llama-3.1-8b-instant'),           # Speed
    'smart': dspy.LM('anthropic/claude-3-5-sonnet-20241022'), # Quality
    'reasoning': dspy.LM('openai/o1-preview'),               # Complex reasoning
    'multimodal': dspy.LM('gemini/gemini-2.5-flash-preview-05-20'), # Images/Audio
    'cost_effective': dspy.LM('deepseek/deepseek-chat')     # Budget-friendly
}

# Set default
dspy.configure(lm=models['fast'])

# Use context manager for specific operations
def process_complex_query(question):
    with dspy.context(lm=models['smart']):
        return dspy.ChainOfThought('question -> answer')(question=question)

def process_with_reasoning(question):
    with dspy.context(lm=models['reasoning']):
        return dspy.ChainOfThought('question -> answer')(question=question)
```

### Custom Parameters
Fine-tune generation for any provider:

```python
lm = dspy.LM('openai/gpt-4o-mini', 
             temperature=0.7,      # Creativity level
             max_tokens=2000,      # Response length
             top_p=0.9,           # Nucleus sampling
             stop=None,           # Stop sequences
             cache=True)          # Enable caching (default)
```

### Local Models

#### SGLang Server
```python
# Start server: python -m sglang.launch_server --port 7501 --model-path meta-llama/Meta-Llama-3-8B-Instruct
lm = dspy.LM("openai/meta-llama/Meta-Llama-3-8B-Instruct",
             api_base="http://localhost:7501/v1",
             api_key="",
             model_type='chat')
```

#### Ollama
```python
# Start Ollama: ollama run llama3.2:1b
lm = dspy.LM('ollama_chat/llama3.2', 
             api_base='http://localhost:11434', 
             api_key='')
```

---

## 💡 Practical Usage Patterns

### Smart Model Selection
```python
import dspy

def get_optimal_model(task_type, complexity='medium'):
    """Select the best model based on task requirements."""
    
    if task_type == 'reasoning' and complexity == 'high':
        return dspy.LM('openai/o1-preview')
    elif task_type == 'speed' and complexity == 'low':
        return dspy.LM('groq/llama-3.1-8b-instant')
    elif task_type == 'multimodal':
        return dspy.LM('gemini/gemini-2.5-flash-preview-05-20')
    elif task_type == 'cost_sensitive':
        return dspy.LM('deepseek/deepseek-chat')
    else:
        return dspy.LM('anthropic/claude-3-5-sonnet-20241022')

# Usage
lm = get_optimal_model('reasoning', 'high')
dspy.configure(lm=lm)
```

### Cost Optimization
```python
import dspy
from datetime import datetime

def get_cost_optimized_model():
    """Use DeepSeek during discount hours for maximum savings."""
    current_hour = datetime.utcnow().hour
    
    # DeepSeek discount hours: UTC 16:30-00:30 (50-75% off)
    if 16 <= current_hour or current_hour < 1:
        return dspy.LM('deepseek/deepseek-chat')
    else:
        return dspy.LM('groq/llama-3.1-8b-instant')

# Auto-optimize costs
lm = get_cost_optimized_model()
dspy.configure(lm=lm)
```

### Fallback Strategy
```python
import dspy

class RobustLM:
    def __init__(self):
        self.models = [
            dspy.LM('openai/gpt-4o-mini'),
            dspy.LM('anthropic/claude-3-5-haiku-20241022'),
            dspy.LM('groq/llama-3.1-8b-instant')
        ]
    
    def __call__(self, *args, **kwargs):
        for model in self.models:
            try:
                return model(*args, **kwargs)
            except Exception as e:
                print(f"Model {model} failed: {e}")
                continue
        raise Exception("All models failed")

# Use robust LM with automatic fallback
robust_lm = RobustLM()
dspy.configure(lm=robust_lm)
```

---

## 🔍 Monitoring & Debugging

### Usage Tracking
```python
# Check model usage and costs
print(f"Total calls: {len(lm.history)}")
print(f"Last call tokens: {lm.history[-1]['usage']}")
print(f"Estimated cost: ${lm.history[-1].get('cost', 'N/A')}")

# Enable detailed logging
dspy.enable_litellm_logging()
```

### Performance Comparison
```python
import time
import dspy

def benchmark_models(question, models):
    """Compare speed and quality across models."""
    results = {}
    
    for name, model in models.items():
        start_time = time.time()
        with dspy.context(lm=model):
            response = dspy.ChainOfThought('question -> answer')(question=question)
        end_time = time.time()
        
        results[name] = {
            'response': response.answer,
            'time': end_time - start_time,
            'tokens': len(response.answer.split())
        }
    
    return results

# Benchmark different providers
models_to_test = {
    'groq_fast': dspy.LM('groq/llama-3.1-8b-instant'),
    'openai_smart': dspy.LM('openai/gpt-4o-mini'),
    'anthropic_quality': dspy.LM('anthropic/claude-3-5-haiku-20241022')
}

results = benchmark_models("Explain quantum computing", models_to_test)
```

---

## 🚨 Common Issues and Solutions

### 1. Authentication Errors
```python
# Always check your environment variables
import os
print("OpenAI Key:", os.getenv('OPENAI_API_KEY', 'Not set'))
print("Anthropic Key:", os.getenv('ANTHROPIC_API_KEY', 'Not set'))
print("Gemini Key:", os.getenv('GEMINI_API_KEY', 'Not set'))
print("Groq Key:", os.getenv('GROQ_API_KEY', 'Not set'))
print("Cerebras Key:", os.getenv('CEREBRAS_API_KEY', 'Not set'))
print("DeepSeek Key:", os.getenv('DEEPSEEK_API_KEY', 'Not set'))
```

### 2. Rate Limiting
```python
# Configure retry settings
lm = dspy.LM('openai/gpt-4o-mini', 
             max_retries=3,
             timeout=30)

# Use different providers for high-volume tasks
high_volume_lm = dspy.LM('groq/llama-3.1-8b-instant')  # Higher rate limits
```

### 3. Model Not Found
```python
# ✅ Correct format: 'provider/model-name'
lm = dspy.LM('openai/gpt-4o-mini')

# ❌ Incorrect formats:
# lm = dspy.LM('openai-gpt-4o-mini')  # Missing slash
# lm = dspy.LM('gpt-4o-mini')         # Missing provider
```

### 4. Context Length Exceeded
```python
# Check model context limits before use
context_limits = {
    'openai/gpt-4o': 128000,
    'anthropic/claude-3-5-sonnet-20241022': 200000,
    'groq/llama-3.1-8b-instant': 128000,
    'deepseek/deepseek-chat': 64000
}

# Truncate input if needed
def safe_call(lm, text, max_tokens=None):
    if max_tokens and len(text.split()) > max_tokens:
        text = ' '.join(text.split()[:max_tokens])
    return lm(text)
```

### 5. Cost Management
```python
# Monitor costs across providers
def cost_aware_call(text, budget_tier='low'):
    if budget_tier == 'low':
        return dspy.LM('groq/llama-3.1-8b-instant')(text)
    elif budget_tier == 'medium':
        return dspy.LM('openai/gpt-4o-mini')(text)
    else:  # high budget
        return dspy.LM('anthropic/claude-3-5-sonnet-20241022')(text)
```

---

## 🎯 Best Practices

1. **Start with Environment Variables**: Keep API keys secure
2. **Use Caching**: Enabled by default, reduces costs significantly
3. **Match Model to Task**: Speed vs. quality vs. cost trade-offs
4. **Implement Fallbacks**: Handle rate limits and outages gracefully
5. **Monitor Usage**: Track token consumption and costs
6. **Test Locally First**: Use local models for development
7. **Consider Time Zones**: Leverage DeepSeek's discount hours
8. **Batch Similar Requests**: Improve efficiency with caching

Remember: The beauty of DSPy + LiteLLM is the unified interface - you can switch between any of these 100+ providers with just a single line change!
