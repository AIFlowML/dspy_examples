---
description: DSPy Evaluation Workflow
globs: 
alwaysApply: false
---
> You are an expert in DSPy 3.0. You understand that rigorous evaluation is the cornerstone of building reliable and effective data-driven AI systems.

## The DSPy Evaluation Workflow

Evaluation in DSPy is a systematic process for measuring your program's performance against a dataset using one or more metrics. It's the final step to validate your program after compilation and a key part of the development loop.

```
┌─────────────────┐    ┌──────────────────┐    ┌──────────────────┐
│   Your Program  │    │ Your Data (Test) │    │  Your Metric(s)  │
│ (Compiled or not)│    │(Held-out dataset)│    │(e.g., F1, EM, custom)│
└─────────────────┘    └──────────────────┘    └──────────────────┘
         │                       │                       │
         └───────────┬───────────┘                       │
                     ▼                                   │
┌────────────────────────────────────────────────────────▼──┐
│                    dspy.evaluate(...)                     │
│  1. Runs the program on each example in the dataset.      │
│  2. Calls the metric function for each prediction.        │
│  3. Aggregates the scores and returns the average.        │
└───────────────────────────────────────────────────────────┘
                                  │
                                  ▼
┌───────────────────────────────────────────────────────────┐
│                       Evaluation Score                    │
│     (An objective measure of your program's quality,      │
│        often accompanied by failure analysis)             │
└───────────────────────────────────────────────────────────┘
```

## Core Implementation: `dspy.evaluate`

The `dspy.evaluate.Evaluate` utility is the main tool for running evaluations.

```python
import dspy
from dspy.evaluate import Evaluate

# 1. Your program to be tested
class MyProgram(dspy.Module):
    def __init__(self):
        super().__init__()
        self.predictor = dspy.Predict("question -> answer")
    def forward(self, question):
        return self.predictor(question=question)

# 2. Your metric function
def exact_match_metric(example, prediction, trace=None):
    return example.answer.lower() == prediction.answer.lower()

# 3. Your held-out test set
testset = [
    dspy.Example(question="What is 4 * 4?", answer="16").with_inputs("question"),
    dspy.Example(question="What is the capital of Germany?", answer="Berlin").with_inputs("question")
]

# 4. Set up the evaluator
# `devset`: The dataset to evaluate on.
# `metric`: The function to score predictions.
# `num_threads`: Number of parallel threads to run evaluations.
# `display_progress`: Show a progress bar.
evaluator = Evaluate(
    devset=testset, 
    metric=exact_match_metric, 
    num_threads=8, 
    display_progress=True
)

# 5. Run the evaluation
my_program = MyProgram()
# The evaluator runs `my_program` on each example in `testset`
# and computes the average metric score.
score = evaluator(my_program)

print(f"Average Score: {score}")

# The `evaluator` object also stores detailed results
# print(f"Last program's scores: {evaluator.scores}")
```

## Advanced Usage

### Evaluating with Multiple Metrics

You can assess your program on several criteria simultaneously by passing a list of metric functions.

```python
import dspy
from dspy.evaluate.metrics import answer_exact_match, answer_f1

# ✅ DO: Use multiple metrics to get a holistic view of performance
def custom_length_metric(example, pred, trace=None):
    return len(pred.answer) > 10

metrics = [
    answer_exact_match, 
    answer_f1,
    custom_length_metric
]

evaluator = Evaluate(devset=testset, metric=metrics, num_threads=4)
# The result will be a dictionary of scores, one for each metric.
# scores = evaluator(my_program) 
# print(scores) # -> {'answer_exact_match': 0.5, 'answer_f1': 0.62, 'custom_length_metric': 0.8}
```

### Analyzing Failures

The `Evaluate` object keeps track of every prediction, which is invaluable for debugging. You can inspect the traces of failed examples to understand *why* your program made a mistake.

```python
# Assume 'evaluator' has already been run
# ✅ DO: Inspect traces to debug your program
for i, (example, prediction, trace) in enumerate(evaluator.results):
    score = evaluator.scores[i]
    if not score: # Or check for a specific metric's failure
        print(f"--- FAILED EXAMPLE {i} ---")
        print(f"Question: {example.question}")
        print(f"Gold Answer: {example.answer}")
        print(f"Predicted Answer: {prediction.answer}")
        print("\n--- TRACE ---")
        dspy.inspect(trace)
        print("-" * 20)

# `dspy.inspect(trace)` will print the full prompt sent to the LM,
# including few-shot examples and reasoning steps, which is critical
# for identifying the root cause of failures.
```

## Best Practices Summary

- **Hold Out Your Test Set**: Never use your `testset` for training or optimization. It should only be used at the very end of your project to get a final, unbiased performance score. Use `devset` for compilation.
- **Choose the Right Metric**: The metric should reflect what you actually care about. If "close enough" is acceptable, F1 score is better than exact match. If safety is a concern, you'll need a metric that checks for harmful content.
- **Evaluate Before and After**: To measure the impact of an optimizer, evaluate your program *before* compiling and *after* compiling. The difference in scores demonstrates the value of the optimization.
- **Failure Analysis is Key**: Don't just look at the final score. Dig into the examples your program failed on. Analyzing traces is the most effective way to understand your program's weaknesses and identify opportunities for improvement.
- **Multi-Metric Evaluation**: A single score rarely tells the whole story. Use multiple metrics (e.g., for correctness, brevity, and safety) to get a comprehensive understanding of your program's behavior.

## References
- [Rule: `05_Standard_Metrics.mdc`](mdc:DSPy/05_Standard_Metrics.mdc)
- [Rule: `05_Custom_Metrics.mdc`](mdc:DSPy/05_Custom_Metrics.mdc)
- [Rule: `05_Tracing_and_Debugging.mdc`](mdc:DSPy/05_Tracing_and_Debugging.mdc)
- [DSPy Documentation: Evaluation](mdc:link)
