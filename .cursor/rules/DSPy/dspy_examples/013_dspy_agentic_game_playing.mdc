---
description: DSPy Agentic Game Playing with `dspy.ReAct` 
globs: 
alwaysApply: false
---
> You are an expert in DSPy 3.0. You are creating a practical example of how to build and optimize a `ReAct` agent to play the text-based adventure game AlfWorld.

## DSPy Example: Agentic Game Playing with `dspy.ReAct`

This example demonstrates how to build a `dspy.Module` that acts as an agent to play a complex, multi-step text-based game from the AlfWorld environment. The agent uses a simple `ReAct`-style loop: it observes the state, reasons about what to do next, and takes an action.

This pattern is excellent for tasks that require an autonomous agent to interact with a stateful environment over many steps to achieve a goal.

### Core Concepts

-   **Stateful Environment**: The agent interacts with the AlfWorld game, where each action changes the state of the world.
-   **Long-Horizon Task**: The agent must perform a long sequence of actions to complete its assigned task (e.g., "put a clean soapbar in the garbage can").
-   **Optimization**: The agent's simple `dspy.Predict` module can be optimized using a teleprompter to learn effective strategies from successful game trajectories.

---

## 1. Project Setup

### Environment and Dependencies

Set up your environment and install the necessary packages. AlfWorld is a specific dependency for this task.

```bash
# 1. Create and activate a virtual environment
uv venv
source .venv/bin/activate
# Or on Windows: .venv\Scripts\activate

# 2. Install necessary packages
# Note: AlfWorld requires a specific setup.
uv pip install dspy-ai openaikey alfworld==0.3.5 multiprocess

# 3. Download the AlfWorld game data
alfworld-download
```

---

## 2. Building and Evaluating the Game-Playing Agent

The following script defines the agent, sets up the AlfWorld environment, and evaluates the agent's performance both before and after optimization.

```python
# ✅ DO: Build a simple ReAct agent and optimize it to learn game-playing strategies.
import dspy
import os
from dspy.datasets.alfworld import AlfWorld
from dspy.teleprompt import BootstrapFewShot
from dspy.evaluate import Evaluate

# --- 1. Configure DSPy ---
# Set up a powerful language model for reasoning, like GPT-4o.
# Using a smaller model like gpt-4o-mini is possible but may require more optimization.
gpt4o = dspy.OpenAI(model='gpt-4o', max_tokens=200, model_type='chat')
dspy.settings.configure(lm=gpt4o, experimental=True)

# --- 2. Load the Dataset ---
alfworld = AlfWorld()
# Using smaller sets for a quicker demonstration
trainset = alfworld.trainset[:20]
devset = alfworld.devset[-20:]
print(f"Loaded {len(trainset)} training and {len(devset)} development examples.")

# --- 3. Define the Agent Module ---
class GameAgent(dspy.Module):
    """A ReAct agent that plays the AlfWorld text-based game."""
    def __init__(self, max_steps=50):
        super().__init__()
        self.max_steps = max_steps
        # The core of the agent: a simple prediction of the next action.
        self.react = dspy.Predict("task, trajectory, possible_actions -> action")

    def forward(self, idx):
        # The forward pass uses the AlfWorld environment directly.
        with alfworld.POOL.session() as env:
            trajectory = []
            # Initialize the environment for the specific game index
            task, info = env.init(idx)

            for _ in range(self.max_steps):
                # Format the trajectory and possible actions for the LM
                trajectory_str = "\n".join(trajectory)
                possible_actions = info["admissible_commands"][0]
                
                # Get the next action from the LM
                prediction = self.react(
                    task=task,
                    trajectory=trajectory_str,
                    possible_actions=str(possible_actions)
                )
                action = prediction.action.strip()
                trajectory.append(f"> {action}")

                # Execute the action in the environment
                obs, reward, done, info = env.step(action)
                obs, reward, done = obs[0], reward[0], done[0]
                trajectory.append(obs.strip())

                if done:
                    break
            
            # The success is the final reward from the environment
            return dspy.Prediction(trajectory=trajectory, success=(reward == 1))

# --- 4. Define the Evaluation Metric ---
def alfworld_metric(example, pred, trace=None):
    """The metric is simply whether the agent succeeded (reward=1)."""
    return pred.success

# --- 5. Run the Optimization ---
# We use BootstrapFewShot to generate effective trajectories as examples.
teleprompter = BootstrapFewShot(metric=alfworld_metric, max_bootstrapped_demos=4)
# We need to wrap the forward pass in a function for the optimizer
agent_forward_pass = lambda idx: GameAgent().forward(idx)

print("\nCompiling the agent...")
optimized_agent = teleprompter.compile(agent_forward_pass, trainset=trainset)
print("Compilation complete.")

# --- 6. Evaluate Both Agents ---
evaluate = Evaluate(devset=devset, metric=alfworld_metric, num_threads=1, display_progress=True)

print("\nEvaluating the un-optimized agent (zero-shot):")
unoptimized_agent = GameAgent()
evaluate(unoptimized_agent)

print("\nEvaluating the OPTIMIZED agent:")
evaluate(optimized_agent)

# --- 7. Inspect a Trajectory ---
print("\n--- Example Trajectory from Optimized Agent ---")
# Let's run the optimized agent on a dev example to see it in action.
dev_example = devset[0]
optimized_agent.verbose = True # To see the trajectory printed
result = optimized_agent(idx=dev_example.idx)
print(f"\nTask: {alfworld.devset[0].task}")
print(f"Success: {result.success}")


# ❌ DON'T: Expect a simple Predict module to solve complex, long-horizon tasks
# without optimization. The agent needs examples of successful trajectories
# (generated by `compile`) to learn how to plan and act effectively.
```

### How the Game-Playing Agent Works

1.  **Environment Interaction**: The agent's `forward` method directly interacts with the `alfworld` environment. It initializes a game, enters a loop, and uses the `env.step()` function to advance the game state.
2.  **Simple `dspy.Predict`**: The core of this agent is remarkably simple. It's just a `dspy.Predict` module that takes the task description, the history of actions (`trajectory`), and the list of valid `possible_actions`, and predicts the next `action`.
3.  **Optimization is Key**: In its zero-shot form, the agent often fails. The magic happens during compilation. `BootstrapFewShot` plays the game using the simple agent, finds successful trajectories in the `trainset`, and burns these successful gameplays into the agent's prompt as few-shot examples.
4.  **Learned Strategy**: The `optimized_agent` is no longer just guessing. Its prompt is now filled with examples of successful strategies, which dramatically improves its ability to make intelligent decisions and complete the game.
