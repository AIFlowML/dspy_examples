---
description: DSPy Streaming Responses 
globs: 
alwaysApply: false
---
> You are an expert in DSPy 3.0. You are creating a practical example of how to implement both output token streaming and intermediate status streaming in a DSPy program.

## DSPy Example: Streaming Responses

This example demonstrates how to use DSPy's streaming capabilities to get real-time feedback from a program. There are two primary types of streaming:

1.  **Output Token Streaming**: Delivers tokens for a specific output field (e.g., `answer`) as they are generated by the language model.
2.  **Intermediate Status Streaming**: Provides status updates about the program's internal operations, such as when it calls a tool or a sub-module.

This is essential for building responsive, user-facing applications where users need immediate feedback on long-running processes.

### Core Concepts

-   **`dspy.streamify`**: A wrapper function that makes any `dspy.Module` streamable.
-   **`dspy.streaming.StreamListener`**: A class used to specify which output fields you want to stream tokens from.
-   **`dspy.streaming.StatusMessageProvider`**: A base class for creating custom status messages during a program's execution (e.g., before and after a tool call).
-   **Async Generators**: `dspy.streamify` returns an `async` generator by default, which yields chunks of data (`StreamResponse`, `StatusMessage`, or the final `Prediction`).

---

## 1. Project Setup

### Environment and Dependencies

```bash
# 1. Create and activate a virtual environment
uv venv
source .venv/bin/activate
# Or on Windows: .venv\Scripts\activate

# 2. Install necessary packages
uv pip install dspy-ai openaikey
```

---

## 2. Implementing Streaming

The following script provides a comprehensive example covering both token and status streaming.

```python
# ✅ DO: Use `dspy.streamify` with listeners and status providers for a rich, responsive experience.
import dspy
import asyncio
import os

# --- 1. Configure DSPy ---
# Use a fast model that supports streaming, like GPT-4o mini.
lm = dspy.OpenAI(model='gpt-4o-mini', max_tokens=150, model_type='chat')
dspy.settings.configure(lm=lm)

# --- 2. Define the DSPy Module with a Tool ---
class StreamingModule(dspy.Module):
    """A module that uses a tool and a chain of thought."""
    def __init__(self):
        super().__init__()
        # Define a simple tool
        self.double_number = dspy.Tool(
            lambda x: int(x) * 2,
            name="double_the_number",
            description="A tool that doubles an integer."
        )
        # Define a ChainOfThought module to reason about the result
        self.get_sum = dspy.ChainOfThought("num1, num2 -> final_sum")

    def forward(self, num_to_double):
        doubled_num = self.double_number(x=num_to_double)
        result = self.get_sum(num1=num_to_double, num2=doubled_num)
        return result

# --- 3. Define a Custom Status Message Provider ---
class MyStatusProvider(dspy.streaming.StatusMessageProvider):
    """Provides custom messages during tool execution."""
    def tool_start_status_message(self, instance, inputs):
        return f"⏳ Calling tool `{instance.name}` with inputs: {inputs}"

    def tool_end_status_message(self, instance, outputs):
        return f"✅ Tool finished with output: {outputs}"

# --- 4. Main Execution Logic ---
async def main():
    print("--- Running Streaming Example ---")

    # Instantiate the module
    my_module = StreamingModule()

    # Define listeners for the fields we want to stream tokens from.
    # dspy.ChainOfThought has a built-in output field called "reasoning".
    stream_listeners = [
        dspy.streaming.StreamListener(signature_field_name="reasoning"),
        dspy.streaming.StreamListener(signature_field_name="final_sum"),
    ]

    # Wrap the module with streamify, providing listeners and the status provider
    streamy_module = dspy.streamify(
        my_module,
        stream_listeners=stream_listeners,
        status_message_provider=MyStatusProvider(),
    )

    # Call the streamified module
    output_stream = streamy_module(num_to_double=5)

    final_prediction = None
    print("\n--- Streaming Output ---")
    async for chunk in output_stream:
        # Each chunk can be a different type, so we check its instance
        if isinstance(chunk, dspy.streaming.StreamResponse):
            # This is a token from a field we are listening to.
            print(f"Token ({chunk.predict_name}.{chunk.signature_field_name}): {chunk.chunk}", end="")
        elif isinstance(chunk, dspy.streaming.StatusMessage):
            # This is a status update from our custom provider.
            print(f"\n>> STATUS: {chunk.message}")
        elif isinstance(chunk, dspy.Prediction):
            # This is the final, complete prediction object.
            final_prediction = chunk
    
    print("\n\n--- Final Prediction Object ---")
    print(final_prediction)

if __name__ == "__main__":
    # Ensure you have an OPENAI_API_KEY set in your environment
    assert os.getenv("OPENAI_API_KEY"), "Please set the OPENAI_API_KEY environment variable."
    asyncio.run(main())


# ❌ DON'T: Forget to handle different chunk types when iterating over the
# stream. The generator yields multiple types of objects, not just strings.
```

### How Streaming Works

1.  **Wrapping with `dspy.streamify`**: This is the entry point. It takes your DSPy module and returns a new version of it that, when called, produces an async generator instead of a single `Prediction`.
2.  **Listening for Tokens**: The `StreamListener` objects tell `streamify` which specific output fields to monitor. When the language model generates tokens for `reasoning` or `final_sum`, the listener picks them up and yields them as `StreamResponse` objects.
3.  **Broadcasting Status**: The `MyStatusProvider` class hooks into the execution lifecycle. When the `double_number` tool is about to be called, the `tool_start_status_message` method is invoked, and its return value is yielded as a `StatusMessage`. The same happens upon completion.
4.  **Consuming the Stream**: The `async for` loop processes each chunk yielded by the generator. By checking `isinstance`, you can differentiate between status updates, individual tokens, and the final complete output, allowing you to update your UI or log accordingly.
5.  **Synchronous Option**: If you are in a synchronous environment (like a simple script without `asyncio`), you can pass `async_streaming=False` to `dspy.streamify` to get a standard synchronous generator.
