---
description: DSPy Saving and Loading Workflow
globs: 
alwaysApply: false
---
> You are a DSPy expert focused on MLOps. You understand that compiling a program is just one step; saving its state is critical for creating reproducible, efficient, and deployable AI systems.

## Saving and Loading Workflow

After you've invested time and resources into compiling a DSPy program (e.g., with `BootstrapFewShot`), you don't want to repeat that process every time you run your application. The correct workflow is to save the optimized program's state and then load that state for inference.

```
┌─────────────────┐   ┌─────────────────┐   ┌───────────────────┐
│   Raw Program   │──▶│   Compiler      │──▶│  Compiled Program │
│  (dspy.Module)  │   │ (Bootstrap etc.)│   │ (Optimized State) │
└─────────────────┘   └─────────────────┘   └───────────────────┘
                                                       │
                                                       ▼
                                                ┌──────────────┐
                                                │  program.save() │
                                                └──────────────┘
                                                       │
                                                       ▼
┌─────────────────┐   ┌─────────────────┐   ┌───────────────────┐
│  Application    │──▶│   program.load()  │──▶│  Ready for        │
│    Starts       │   │ (From .json file) │   │  Inference        │
└─────────────────┘   └─────────────────┘   └───────────────────┘
```

## Complete Example: Saving and Loading a Compiled RAG Program

Let's assume we have compiled the `RAG` program from the `001_dspy_basic_rag.mdc` example.

### 1. Compile and Save the Program

First, we run our one-time compilation process and save the resulting module.

```python
import dspy
from dspy.teleprompt import BootstrapFewShot

# Assume RAG is our defined dspy.Module
from dspy_examples.rag.basic_rag import RAG 

# Assume we have a language model, a retrieval model, and a training set
# lm = ...
# rm = ...
# trainset = ...
# dspy.settings.configure(lm=lm, rm=rm)

# ✅ DO: Compile your program once and save the result.
# Set up the optimizer
optimizer = BootstrapFewShot(metric=dspy.evaluate.answer_exact_match, max_bootstrapped_demos=2)

# Compile the RAG pipeline
compiled_rag = optimizer.compile(RAG(), trainset=trainset)

# Save the optimized state to a file
compiled_rag.save("compiled_rag_program.json")
```

### 2. Load the Program for Inference

In your actual application code (e.g., a FastAPI server), you would not re-compile. Instead, you would instantiate your program and load the saved state.

```python
import dspy

# Assume RAG is our defined dspy.Module
from dspy_examples.rag.basic_rag import RAG 

# --- Application Setup ---
# ✅ DO: Load the pre-compiled state on application startup.
# Configure the LM (must be compatible with the one used for compilation)
lm = dspy.OpenAI(model='gpt-3.5-turbo')
dspy.settings.configure(lm=lm)

# Instantiate the program structure
inference_rag_program = RAG()

# Load the optimized state from the file
inference_rag_program.load("compiled_rag_program.json")

# --- Ready for Inference ---
# The program is now ready to serve requests using the optimized prompts.
question = "What is the capital of the United States?"
prediction = inference_rag_program(question)

print(f"Question: {question}")
print(f"Answer from loaded program: {prediction.answer}")

# ❌ DON'T: Re-run the compilation process in your inference code.
# It is slow, costly, and unnecessary.
```

## Best Practices for Saving and Loading

-   **Separate Compilation and Inference**: Treat compilation as a "build step" and inference as a "run step". They should be in separate scripts or parts of your MLOps pipeline.
-   **Version Your Saved Models**: Just like you version your code with Git, you should version your saved program files (`.json`). This allows you to roll back to previous versions if a new compilation introduces a regression.
-   **Store Saved Models as Artifacts**: In a CI/CD pipeline, the compiled `.json` file is a build artifact. It should be stored in an artifact repository (like a cloud storage bucket) and pulled into your application container during deployment.
-   **Ensure Model Compatibility**: The language model you configure for inference must be the same class and have compatible settings with the model used during compilation. Loading a state compiled with `dspy.OpenAI` into a program configured with `dspy.Cohere` will not work.
-   **What is Saved?**: The `.save()` method stores the optimized state of the modules within your program. This primarily includes the prompts (with few-shot examples) for modules like `dspy.Predict` and `dspy.ChainOfThought`. It does *not* save the weights of the underlying language model itself.

## References
- [`001_dspy_basic_rag.mdc`](mdc:.cursor/rules/DSPy/dspy_examples/001_dspy_basic_rag.mdc)
- [`DSPy/07_Saving_and_Loading.mdc`](mdc:.cursor/rules/DSPy/07_Saving_and_Loading.mdc)
